---
title: "ML 2 Assignment 2"
author: "Gerold Csendes"
date: '2020 03 08 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r lib, echo=FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(caret)
library(ISLR)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(xgboost)
library(randomForest)
library(gbm)
library(MLmetrics)
library(kableExtra)
library(pROC)
library(plotROC)
```

## 1. Tree ensemble models

> a. Create a training data of 75% and keep 25% of the data as a test set. Train a decision tree as a benchmark model. Plot the final model and interpret the result. 

In contrast to `caret's` default performance metric, Accuracy, I will use the AUC as the metric for evaluation, since this is a more robust method.

```{r 1_a}
set.seed(1)
# create train/test
train_indices <- createDataPartition(
  y = data[["Purchase"]],
  times = 1,
  p = 0.75,
  list = F
  
)
data_train <- data[train_indices,]
data_test <- data[-train_indices, ]

# set CV as train control
train_control <- trainControl(
  method = "cv",
  number = 3,
  classProbs = TRUE,
  summaryFunction = multiClassSummary # needed for AUC metric
)

# set up a tune grid
tree_grid <- data.frame(
  cp = seq(0, 0.15, 0.01)
)

# train model
set.seed(1)
simple_tree_model <- train(
  Purchase ~ .,
  method = "rpart",
  data = data_train,
  tuneGrid = tree_grid,
  trControl = train_control,
  metric = 'AUC'
)
# plot model
rpart.plot(simple_tree_model[["finalModel"]])
```

It seems that after controlling for one of the hyperparameters - __complexity__ - of `Rpart`, we get the above tree description. The ideal __cp__ paratemeer is 0 which suggests that the trees didn't overfit. The tree seems to me too complex for this size of a dataset but it did end up somehow being the best-performing. 

> b. Investigate tree ensemble models: random forest, gradient boosting machine, XGBoost. Try various tuning parameter combinations and select the best model using cross-validation. 

```{r 1_b}
# train RANDOM FOREST
# grid newly set after a few tries
rf_grid <- expand.grid(
  mtry = c(2, 3, 5, 7, 8, 9),
  splitrule = "gini",
  min.node.size = c(3, 5, 7, 8, 9, 10, 11)
)

set.seed(1)
rf_model <- train(Purchase ~ .,
                  method = "ranger",
                  data=data_train,
                  trControl = train_control,
                  tuneGrid = rf_grid,
                  verbose = FALSE,
                  metric = "AUC",
                  importance = 'impurity') # needed varImp

# Train GBM
gbm_grid <- expand.grid(n.trees = c(100, 300, 500), 
                        interaction.depth = c(2, 3, 5), 
                        shrinkage = c(0.005, 0.01, 0.1),
                        n.minobsinnode = c(3, 5, 7, 9, 11))
set.seed(1)
gbm_model <- train(Purchase ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid,
                   verbose = FALSE, 
                   metric = 'AUC'
                   )
# Train XGBoost
xgb_grid <- expand.grid(nrounds = c(500, 700),
                       max_depth = c(3, 4, 5),
                       eta = c(0.01, 0.05),
                       gamma = 0,
                       colsample_bytree = c(0.5, 0.7),
                       min_child_weight = c(3, 5), # similar to n.minobsinnode
                       subsample = c(0.5))
xgb_model <- train(Purchase ~.,
                   method = "xgbTree",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = xgb_grid,
                   verbose = F,
                   metric = 'AUC')
```

> c. Compare different models with the resamples function (make sure to set the same seed before model training for all 3 models). Is any of these giving significantly different predictive power than the others? 

```{r 1_c}
resamp <- resamples(list(Tree = simple_tree_model,
                         Rforest = rf_model,
                         GBM = gbm_model,
                         XGBM = xgb_model))
resamp_summary <- summary(resamp)
resamp_summary <- as.data.frame(resamp_summary$statistics$AUC)
resamp_summary$`NA's` <- NULL

# make it look nice
kable(resamp_summary, caption = "Model performances", digits = 3) %>% 
    kable_styling(latex_options = "hold_position")
```

Well, it can be clearly seen that the __simple__ tree is inferior compared to the other models performances. One may argue that the difference in performance between random forest and the boosting methods are significant. However, I don't thik it would be reasonable to choose XGBM over GBM for such a 0.005 better AUC.

> d. Choose the best model and plot ROC curve for the best model on the test set. Calculate and interpret AUC. 

Since there is no practical constraint on choosing between GBM and XGBM, I tossed a coin which favoured GBM so I will go with that model.

```{r 1_d}
# retrain gbm on whole train
gbm_grid_final <- gbm_model$bestTune
gbm_model_final <- train(Purchase ~ .,
                   method = "gbm",
                   data = data_train,
                   trControl = train_control,
                   tuneGrid = gbm_grid_final,
                   verbose = FALSE, 
                   metric = 'AUC'
                   )

# Make predictions
data_test[, c('gbm_pred_ch', 'gbm_pred_mm') := predict(gbm_model, newdata = data_test, type="prob")]
data_test[, ch_taget := ifelse(Purchase == "CH", 1, 0)]

# plot
rocplot <- ggplot(data_test, aes(m = gbm_pred_ch, d = ch_taget))+ geom_roc(n.cuts=20,labels=FALSE)
rocplot + 
  style_roc(theme = theme_grey) + 
  geom_rocci(fill="pink") +
  labs(title=paste0("AUC: ", round(calc_auc(rocplot)$AUC, 4)))
```

This ROC curve looks satisfying. The AUC of 0.9 shows that our predictions are credible. To be more precise, it means that our model performs at 90% out of the 100. A bigger AUC means that, on average, ther is less trade-off between FPR and TPR.

> e. Inspect variable importance plots for the 3 models. Are similar variables found to be the most important for the 3 models? 

```{r 1_e}
# Tree
varImp(simple_tree_model)
# Random forest
varImp(rf_model)
# GBM
varImp(gbm_model)
# XGBM
varImp(xgb_model)
```

There isn't much difference between the top 5 most important features across models.

## 2. Variable importance profiles 

> a. Train two random forest models: one with mtry = 2 and another with mtry = 10 (use the whole dataset and donâ€™t use cross-validation). Inspect variable importance profiles. What do you see in terms of how important the first few variables are relative to each other? 

```{r 2_a}
# set up data
data <- data.table(Hitters)
data <- data[!is.na(Salary)]
data[, log_salary := log(Salary)]
data[, Salary := NULL]

# define new grid mtry = 2
rf_mtry_2 <- expand.grid(mtry = c(2),
                       splitrule = 'variance',
                       min.node.size = c(1))
rf2_2 <- train(log_salary ~.,
                   data = data,
                   method = "ranger",
                   tuneGrid = rf_mtry_2,
                   importance = "impurity")
# define new grid mtry = 10
rf_mtry_10 <- expand.grid(mtry = c(10),
                       splitrule = 'variance',
                       min.node.size = c(1))
rf2_10 <- train(log_salary ~.,
                   data = data,
                   method = "ranger",
                   tuneGrid = rf_mtry_10,
                   importance = "impurity")

varimp_mtry2 <- varImp(rf2_2)
varimp_mtry2 <- varimp_mtry2$importance
varimp_mtry2$var <- row.names(varimp_mtry2)

varimp_mtry10 <- varImp(rf2_10)
varimp_mtry10 <- varimp_mtry10$importance
varimp_mtry10$var <- row.names(varimp_mtry10)

rf_varimp <- merge(varimp_mtry2, varimp_mtry10, by = "var", 
                   suffixes = c('_mtry2', '_mtry10'))
rf_varimp <- rf_varimp[order(-rf_varimp$Overall_mtry10),]
kable(rf_varimp, caption = "Random Forest variable importances", digits = 2) %>% 
    kable_styling(latex_options = "hold_position")
```

You can see that the top 5 most important features are the same except for the __years__ variable that is the 6th most important feature for __mtry2__ but is only the 13th for __mtry10__. We can also see that variable importance values are a lot bigger for the smaller random parameter tuning. My intuition behind this phenomenon is that when using a smaller random parameter, it is less likely that we will pick a relevant (important) variable. Thus, the more important features may be less in number, resulting in their increased contribution to performance. 

> b. One of them is more extreme in terms of how the most important and the next ones relate to each other. Give an intuitive explanation how mtry relates to relative importance of variables in random forest models. 

Variable __CAtBat__ is by far the most important feature when mtry is 10. I suspect that this variable is not only important but also less correlated with the other important variables. Thus, even when the tree contains other important features, __CAtBat__ can still enhance performance significantly.

> c. In the same vein, estimate two gbm models and set bag.fraction to 0.1 first and to 0.9 in the second. The tuneGrid should consist of the same values for the two models (a dataframe with one row): n.trees = 500, interaction.depth = 5, shrinkage = 0.1, n.minobsinnode = 5. Compare variable importance plots for the two models. What is the meaning of bag.fraction? Based on this, why is one variable importance profile more extreme than the other?

```{r 2_c}
gbm_grid_fraction <- expand.grid(n.trees = c(500), 
                        interaction.depth = c(5), 
                        shrinkage = c(0.1),
                        n.minobsinnode = c(5))

# Train gbm w. bag.fraction = 0.1
set.seed(1)
gbm_model_01 <- train(log_salary ~.,
                   method = "gbm",
                   data = data,
                   tuneGrid = gbm_grid_fraction,
                   bag.fraction = 0.1,
                   verbose = FALSE)
# Train gbm w. bag.fraction = 0.9
set.seed(1)
gbm_model_09 <- train(log_salary ~.,
                   method = "gbm",
                   data = data,
                   tuneGrid = gbm_grid_fraction,
                   bag.fraction = 0.9,
                   verbose = FALSE, 
                   metric = 'RMSE'
                   )

# variable importances
varimp_fraction01 <- varImp(gbm_model_01)$importance
varimp_fraction01$var <- row.names(varimp_fraction01)

varimp_fraction09 <- varImp(gbm_model_09)$importance
varimp_fraction09$var <- row.names(varimp_fraction09)

gbm_varimp <- merge(varimp_fraction01, varimp_fraction09, by = "var", 
                   suffixes = c('_fraction01', '_fraction09'))
gbm_varimp <- gbm_varimp[order(-gbm_varimp$Overall_fraction09),]
rownames(gbm_varimp) <- seq(1:dim(gbm_varimp)[1])

kable(gbm_varimp, caption = "GBM variable importances", digits = 2) %>% 
    kable_styling(latex_options = "hold_position") %>% 
    row_spec(c(1, 2, 7, 11, 12), color = "white", background = "#D7261E")

```

Based on the above table, the variable importance outputs are not that alike anymore. I highlighted the variables being completely different in terms of importance for the two differeting parameterization. Again, __CAtBat__ is the variable that is more extreme in one of the profiles, namely when fraction = 0.9.  

## 3. Stacking 

```{r 3_0, echo=FALSE, message=FALSE}
data <- fread("../data/medical-appointments-no-show/no-show-data.csv")

# some data cleaning
data[, c("PatientId", "AppointmentID", "Neighbourhood") := NULL]
setnames(data, 
         c("No-show", 
           "Age", 
           "Gender",
           "ScheduledDay", 
           "AppointmentDay",
           "Scholarship",
           "Hipertension",
           "Diabetes",
           "Alcoholism",
           "Handcap",
           "SMS_received"), 
         c("no_show", 
           "age", 
           "gender", 
           "scheduled_day", 
           "appointment_day",
           "scholarship",
           "hypertension",
           "diabetes",
           "alcoholism",
           "handicap",
           "sms_received"))

# for binary prediction, the target variable must be a factor
data[, no_show := factor(no_show, levels = c("Yes", "No"))]
data[, handicap := ifelse(handicap > 0, 1, 0)]

# create new variables
data[, gender := factor(gender)]
data[, scholarship := factor(scholarship)]
data[, hypertension := factor(hypertension)]
data[, alcoholism := factor(alcoholism)]
data[, handicap := factor(handicap)]

data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]

# clean up a little bit
data <- data[age %between% c(0, 95)]
data <- data[days_since_scheduled > -1]
data[, c("scheduled_day", "appointment_day", "sms_received") := NULL]
```


> 1. Create train / validation / test sets, cutting the data into 5% - 45% - 50% parts. 

```{r 3_1, message=FALSE}
library(h2o)
h2o.init()
h2o.no_progress()
# convert data to h2o object
h2o_data <- as.h2o(data)

# split data
splitted_data <- h2o.splitFrame(h2o_data, 
                                ratios = c(0.05, 0.45), 
                                seed = 1)
data_train <- splitted_data[[1]]
data_valid <- splitted_data[[2]]
data_test <- splitted_data[[3]]
```

> 2. Train a benchmark model of your choice using h2o (such as random forest, gbm or glm) and evaluate it on the validation set. 

My favorite benchmark model is always regression, that is easy to interpret and trade-off between interpretability and performance can be directly inspected.

```{r 3_2}
y <- "no_show"
X <- setdiff(names(h2o_data), y)
glm_params <- list(alpha = c(0, .25, .5, .75, .1))

# build grid search with previously selected hyperparameters
glm_grid  <- h2o.grid(
              x = X, y = y, 
              algorithm = "glm", 
              training_frame = data_train,
              family = 'binomial', # use this for logistic regression  
              hyper_params = glm_params,
              lambda_search = T,
              nfolds = 5, 
              seed = 1,
              keep_cross_validation_predictions=TRUE)

glm_grid <- h2o.getGrid(grid_id  = glm_grid@grid_id, sort_by = "auc", decreasing = T)
# get the best model based on the cross-validation exercise
best_glm <- h2o.getModel(glm_grid@model_ids[[1]])
best_glm@model$model_summary
```

> 3. Build at least 4 models of different families using cross validation, keeping cross validated predictions. One of the model families must be deeplearning (you can try, for example, different network topologies). 

```{r 3_3, message = FALSE}
# RF train
rf_params <- list(ntrees = c(500),
                  mtries = c(2, 3),
                  min_rows = c(5, 10))
rf_grid <- h2o.grid(x = X, 
                    y = y, 
                    training_frame = data_train, 
                    algorithm = "randomForest", 
                    nfolds = 5,
                    seed = 1,
                    hyper_params = rf_params,
                    keep_cross_validation_predictions=TRUE)
# order by auc
rf_grid <- h2o.getGrid(grid_id  = rf_grid@grid_id, sort_by = "auc", decreasing = T)

# GBM train
gbm_params <- list(learn_rate = c(0.05),
                    max_depth = c(2, 5),
                    sample_rate = c(0.5),
                    col_sample_rate = c(0.5, 1.0))

gbm_grid <- h2o.grid(x = X, 
                     y = y, 
                     training_frame = data_train, 
                     algorithm = "gbm", 
                     nfolds = 5,
                     seed = 1,
                     ntrees = 200, # reduced for reasonable knitting time
                     hyper_params = gbm_params,
                     keep_cross_validation_predictions=TRUE)
# order by auc
gbm_grid <- h2o.getGrid(grid_id  = gbm_grid@grid_id, sort_by = "auc", decreasing = T)

# DL train
dl_model <- h2o.deeplearning(
                x = X, y = y, 
                training_frame = data_train,
                seed = 1,
                hidden = c(32, 32),
                nfolds = 5, 
                keep_cross_validation_predictions = TRUE)

# get best models
glm_model <- h2o.getModel(h2o.getGrid(glm_grid@grid_id)@model_ids[[1]])
rf_model <- h2o.getModel(h2o.getGrid(rf_grid@grid_id)@model_ids[[1]])
gbm_model <- h2o.getModel(h2o.getGrid(gbm_grid@grid_id)@model_ids[[1]])

```


> 4. Evaluate validation set performance of each model. 

```{r 3_4}
# predict on validation set
validation_performances <- list(
  "glm" = h2o.auc(h2o.performance(glm_model, newdata = data_valid)),
  "rf" = h2o.auc(h2o.performance(rf_model, newdata = data_valid)),
  "gbm" = h2o.auc(h2o.performance(gbm_model, newdata = data_valid)),
  "dl" = h2o.auc(h2o.performance(dl_model, newdata = data_valid))
)

validation_performances
```

GBM performs the best followed by the random forest, deep learning and glm. I expected dl to be closer to gbm, though I didn't do any tuning for that model.

> 5. How large are the correlations of predicted scores of the validation set produced by the base learners?

```{r 3_5}

glm.valid_fit = h2o.predict(object = glm_model, newdata = data_valid)
rf.valid_fit = h2o.predict(object = rf_model, newdata = data_valid)
gbm.valid_fit = h2o.predict(object = gbm_model, newdata = data_valid)
dl.valid_fit = h2o.predict(object = dl_model, newdata = data_valid)

valid_df_base <- data.frame(
    pred_glm = as.data.frame(glm.valid_fit)$No,
    pred_rf = as.data.frame(rf.valid_fit)$No,
    pred_gbm = as.data.frame(gbm.valid_fit)$No,
    pred_dl = as.data.frame(dl.valid_fit)$No
)
library(GGally)
ggcorr(valid_df_base, label = TRUE, label_round = 2)
```

It is interesting to see how correlated are the predicitons of random forest, gbm and deep learning while glm remains only moderately correlated.

> 6. Create a stacked ensemble model from the base learners. Experiment with at least two different ensembling meta learners. 

```{r 3_6}
# use glm meta
stacked_model_glm <- h2o.stackedEnsemble(
  X, y,
  metalearner_algorithm = "glm",
  training_frame = data_train,
  base_models = list(glm_model, 
                     rf_model,
                     gbm_model,
                     dl_model))

# use gbm meta
stacked_model_gbm <- h2o.stackedEnsemble(
  X, y,
  training_frame = data_train,
  metalearner_algorithm = "gbm",
  base_models = list(glm_model, 
                     rf_model,
                     gbm_model,
                     dl_model))
```

> 7. Evaluate ensembles on validation set. Did it improve prediction? 

```{r 3_7}
validation_performances_stacked <- list(
  "stacked_glm" = h2o.auc(h2o.performance(stacked_model_glm, newdata = data_valid)),
  "stacked_gbm" = h2o.auc(h2o.performance(stacked_model_gbm, newdata = data_valid))
)
```

> 8. Evaluate the best performing model on the test set. How does performance compare to that of the validation set?

The stacked glm model is only really slightly better than the base gbm model. The difference os only 0.0002 that is neglectable. I will thus use gbm for evaluation on the training set.

```{r 3_8}
validation_performances
validation_performances_stacked

# evaluation on test set
h2o.auc(h2o.performance(gbm_model, newdata = data_test))
```

The test error is somewhat smaller than the validation but still looks okay to me.
